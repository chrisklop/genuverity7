<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YouTube Radicalization Pipeline: Algorithmic Rabbit Holes Examined | GenuVerity</title>
    <meta name="description" content="Analysis of YouTube's recommendation algorithm and its role in radicalization, examining academic research, platform responses, and the contested 'rabbit hole' hypothesis.">
    <meta name="keywords" content="YouTube radicalization, algorithm, rabbit hole, extremism, recommendation system, alternative influence network">
    <meta name="author" content="GenuVerity Intelligence">
    <meta name="robots" content="index, follow, max-image-preview:large">

    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/png" href="../favicon.png">
    <link rel="apple-touch-icon" href="../favicon.png">

    <meta property="og:type" content="article">
    <meta property="og:url" content="https://www.genuverity.com/localreports/youtube-radicalization-pipeline.html">
    <link rel="canonical" href="https://www.genuverity.com/localreports/youtube-radicalization-pipeline.html">
    <meta property="og:title" content="YouTube Radicalization Pipeline: Algorithmic Rabbit Holes Examined | GenuVerity">
    <meta property="og:description" content="Analysis of YouTube's recommendation algorithm and its role in radicalization.">
    <meta property="og:image" content="https://www.genuverity.com/images/thumbnails/youtube-radicalization-pipeline.webp">

    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://www.genuverity.com/localreports/youtube-radicalization-pipeline.html">
    <meta property="twitter:title" content="YouTube Radicalization Pipeline: Algorithmic Rabbit Holes Examined | GenuVerity">
    <meta property="twitter:description" content="Analysis of YouTube's recommendation algorithm and its role in radicalization.">
    <meta property="twitter:image" content="https://www.genuverity.com/images/thumbnails/youtube-radicalization-pipeline.webp">

    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "headline": "YouTube Radicalization Pipeline: Algorithmic Rabbit Holes Examined",
      "description": "Analysis of YouTube's recommendation algorithm and its role in radicalization.",
      "image": ["https://www.genuverity.com/images/thumbnails/youtube-radicalization-pipeline.webp"],
      "datePublished": "2025-12-31",
      "dateModified": "2025-12-31",
      "author": {"@type": "Organization", "name": "GenuVerity", "url": "https://www.genuverity.com"},
      "publisher": {"@type": "Organization", "name": "GenuVerity", "url": "https://www.genuverity.com", "logo": {"@type": "ImageObject", "url": "https://www.genuverity.com/favicon.png"}},
      "mainEntityOfPage": {"@type": "WebPage", "@id": "https://www.genuverity.com/localreports/youtube-radicalization-pipeline.html"},
      "articleSection": "Platform Analysis",
      "keywords": "YouTube, radicalization, algorithm, extremism, recommendation system"
    }
    </script>

    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="../js/chart-watermark.js"></script>
    <script src="../js/chart-defaults.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.4.1/html2canvas.min.js"></script>
    <script src="../js/copyable-sections.js?v=4.0" defer></script>
    <script src="../js/shared-components.js?v=4.0" defer></script>

    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../css/shared-components.css?v=4.0">
    <link rel="stylesheet" href="../css/reports.css?v=4.0">
</head>

<body>
    <div id="navbar-placeholder" data-page-type="report"></div>

    <div class="container">
        <div class="report-meta">
            <span class="meta-tag">Platform Analysis</span>
            <span class="meta-tag red">Algorithmic Amplification</span>
            <span class="meta-tag"><i data-lucide="clock" style="width:12px; display:inline-block; vertical-align:middle;"></i> 20 MIN READ</span>
        </div>

        <h1 class="report-title">YouTube Radicalization Pipeline: Algorithmic Rabbit Holes Examined</h1>
        <h2 class="report-subtitle">What Research Actually Shows About Recommendations, Extremism, and Platform Accountability</h2>

        <div class="content-grid">
            <aside class="sources-sidebar">
                <div class="share-section">
                    <div class="share-heading"><i data-lucide="share-2" style="width:18px;"></i>Share This Report</div>
                    <div class="share-buttons">
                        <button onclick="shareToTwitter()" class="share-btn share-twitter"><i data-lucide="twitter"></i> X</button>
                        <button onclick="shareToFacebook()" class="share-btn share-facebook"><i data-lucide="facebook"></i> Facebook</button>
                        <button onclick="shareToLinkedIn()" class="share-btn share-linkedin"><i data-lucide="linkedin"></i> LinkedIn</button>
                        <button onclick="copyShareLink()" class="share-btn share-copy" id="copyLinkBtn"><i data-lucide="link"></i> Copy Link</button>
                    </div>
                </div>

                <div class="sources-header">
                    <div class="sources-title"><i data-lucide="database" style="width:18px;"></i>Sources First</div>
                    <span class="sources-count">12</span>
                </div>
                <div class="sources-list" id="sourcesList">
                    <a href="https://www.pnas.org/doi/10.1073/pnas.2213020120" target="_blank" class="source-card" id="source-1">
                        <span class="source-ref">1</span>
                        <div><div>Limited Algorithm Effects on Beliefs - PNAS 2025</div><div style="font-size:0.7em; opacity:0.7;">pnas.org</div></div>
                    </a>
                    <a href="https://arxiv.org/abs/1912.11211" target="_blank" class="source-card" id="source-2">
                        <span class="source-ref">2</span>
                        <div><div>Algorithmic Extremism Study - Ledwich & Zaitsev</div><div style="font-size:0.7em; opacity:0.7;">arxiv.org</div></div>
                    </a>
                    <a href="https://datasociety.net/library/alternative-influence/" target="_blank" class="source-card" id="source-3">
                        <span class="source-ref">3</span>
                        <div><div>Alternative Influence Network - Data & Society</div><div style="font-size:0.7em; opacity:0.7;">datasociety.net</div></div>
                    </a>
                    <a href="https://news.northeastern.edu/2024/05/21/youtube-extremism-research/" target="_blank" class="source-card" id="source-4">
                        <span class="source-ref">4</span>
                        <div><div>Off-Platform Viewing Dominates - Northeastern 2024</div><div style="font-size:0.7em; opacity:0.7;">news.northeastern.edu</div></div>
                    </a>
                    <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7613872/" target="_blank" class="source-card" id="source-5">
                        <span class="source-ref">5</span>
                        <div><div>Systematic Review of 23 Studies - PMC</div><div style="font-size:0.7em; opacity:0.7;">pmc.ncbi.nlm.nih.gov</div></div>
                    </a>
                    <a href="https://transparencyreport.google.com/youtube-policy/featured-policies/violent-extremism?hl=en" target="_blank" class="source-card" id="source-6">
                        <span class="source-ref">6</span>
                        <div><div>YouTube Violent Extremism Report - Google</div><div style="font-size:0.7em; opacity:0.7;">transparencyreport.google.com</div></div>
                    </a>
                    <a href="https://dl.acm.org/doi/10.1145/3351095.3372879" target="_blank" class="source-card" id="source-7">
                        <span class="source-ref">7</span>
                        <div><div>Auditing Radicalization Pathways - ACM FAccT</div><div style="font-size:0.7em; opacity:0.7;">dl.acm.org</div></div>
                    </a>
                    <a href="https://arxiv.org/abs/2003.03318" target="_blank" class="source-card" id="source-8">
                        <span class="source-ref">8</span>
                        <div><div>Conspiracy Promotion Analysis - Faddoul et al.</div><div style="font-size:0.7em; opacity:0.7;">arxiv.org</div></div>
                    </a>
                    <a href="https://www.pbs.org/wnet/amanpour-and-company/video/caleb-cain-and-kevin-roose-on-radicalization-and-youtube/" target="_blank" class="source-card" id="source-9">
                        <span class="source-ref">9</span>
                        <div><div>Caleb Cain Case Study - Kevin Roose/NYT</div><div style="font-size:0.7em; opacity:0.7;">pbs.org</div></div>
                    </a>
                    <a href="https://link.springer.com/article/10.1007/s13278-024-01394-8" target="_blank" class="source-card" id="source-10">
                        <span class="source-ref">10</span>
                        <div><div>Radicalization Detection Framework - Springer 2024</div><div style="font-size:0.7em; opacity:0.7;">link.springer.com</div></div>
                    </a>
                    <a href="https://www.commerce.senate.gov/2019/9/mass-violence-extremism-and-digital-responsibility" target="_blank" class="source-card" id="source-11">
                        <span class="source-ref">11</span>
                        <div><div>Senate Hearing on Extremism - Commerce Committee</div><div style="font-size:0.7em; opacity:0.7;">commerce.senate.gov</div></div>
                    </a>
                    <a href="https://arxiv.org/pdf/2203.10666" target="_blank" class="source-card" id="source-12">
                        <span class="source-ref">12</span>
                        <div><div>Algorithmic Debiasing Research - arXiv 2022</div><div style="font-size:0.7em; opacity:0.7;">arxiv.org</div></div>
                    </a>
                </div>
            </aside>

            <article class="article-content">

                <div class="info-box amber copyable-section" id="tldr">
                    <div class="info-title"><i data-lucide="zap"></i>TL;DR</div>
                    <p><strong>MIXED EVIDENCE</strong></p>
                    <p>The "YouTube radicalization pipeline" hypothesis is <strong>more contested than popular narratives suggest</strong>. A 2025 PNAS study with 9,000 participants found algorithmic recommendations had <em>limited impact</em> on political beliefs. However, research also documents real pathways: user migration patterns show audiences moving from moderate to extreme content, and <strong>off-platform viewing</strong> (embedded videos on partisan websites) may drive more radicalization than the algorithm itself.</p>
                </div>

                <div class="info-box copyable-section" id="executive-summary">
                    <div class="info-title"><i data-lucide="file-text"></i>Executive Summary</div>
                    <p>This report synthesizes academic research on YouTube's recommendation algorithm and radicalization. We find the evidence is mixed: 23 studies reviewed show 14 implicating the recommender system, 7 with mixed results, and 2 showing no effect. Recent experimental research (2025) suggests the algorithm's direct influence is limited, but network analysis demonstrates real user migration patterns toward extreme content. The most significant finding may be that off-platform exposure&mdash;extremist videos embedded on partisan websites&mdash;drives more radicalization than YouTube's internal recommendations.</p>
                </div>

                <figure class="float-figure copyable-section" id="studies-chart-wrapper">
                    <div class="chart-wrapper">
                        <div class="chart-header">
                            <div class="chart-title">Research Findings on YouTube Radicalization</div>
                        </div>
                        <canvas id="studiesChart" height="220"></canvas>
                    </div>
                    <figcaption>Systematic review of 23 studies shows mixed evidence (PMC 2021)</figcaption>
                </figure>

                <section class="copyable-section">
                    <h2>The Contested Hypothesis</h2>
                    <p>In March 2018, sociologist <strong>Zeynep Tufekci</strong> called YouTube <em>"one of the most powerful radicalizing instruments of the 21st century"</em> in a widely-cited New York Times opinion piece. She described how regardless of political content searched, recommendations escalated to extreme material&mdash;white supremacist rants, Holocaust denial. This framing shaped public discourse around YouTube's algorithm.</p>

                    <p>However, the academic evidence is more nuanced. A foundational 2019 study by <strong>Ledwich and Zaitsev</strong> analyzing ~800 political channels found data suggesting YouTube's algorithm actually <em>discourages</em> visits to radical content, favoring mainstream media over independent channels. <a href="#source-2" class="citation-link" onclick="highlightSource(event, 'source-2')">[2]</a> This contradicted the "rabbit hole" narrative.</p>

                    <p>A systematic review of 23 studies found <strong>mixed results</strong>: 14 studies implicated the recommender system in problematic pathways, 7 produced mixed findings, and 2 did not implicate the system. <a href="#source-5" class="citation-link" onclick="highlightSource(event, 'source-5')">[5]</a> The variation stems largely from methodological differences&mdash;some studies analyze recommendations in isolation, others track actual user viewing patterns.</p>
                </section>

                <section class="copyable-section">
                    <h2>The 2025 PNAS Breakthrough</h2>
                    <p>The most rigorous recent study, published in the <em>Proceedings of the National Academy of Sciences</em> in 2025, conducted <strong>four experiments with nearly 9,000 participants</strong> using a custom-built interface serving real YouTube videos and recommendations. <a href="#source-1" class="citation-link" onclick="highlightSource(event, 'source-1')">[1]</a></p>

                    <p>Key findings from the University of Pennsylvania research team:</p>
                    <ul>
                        <li>Algorithmic recommendations had <strong>limited impact on political beliefs</strong> and viewing behavior</li>
                        <li>Users gravitated toward content matching existing beliefs <em>regardless of algorithm</em></li>
                        <li><strong>"Rabbit holes" were not found to be extremizing</strong> for most users</li>
                        <li>One exception: conservatives moved slightly rightward in response to recommendations</li>
                    </ul>

                    <p>This suggests the algorithm may amplify existing preferences rather than create radicalization&mdash;a meaningful distinction for policy responses.</p>
                </section>

                <div class="data-table-container copyable-section">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Study</th>
                                <th>Finding</th>
                                <th>Year</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>PNAS (UPenn)</td>
                                <td>Limited algorithm effects on beliefs</td>
                                <td>2025</td>
                            </tr>
                            <tr>
                                <td>Ledwich & Zaitsev</td>
                                <td>Algorithm discourages extremism</td>
                                <td>2019</td>
                            </tr>
                            <tr>
                                <td>ACM FAccT Audit</td>
                                <td>User migration patterns toward extremes</td>
                                <td>2020</td>
                            </tr>
                            <tr>
                                <td>Northeastern/Wilson</td>
                                <td>Off-platform viewing drives exposure</td>
                                <td>2024</td>
                            </tr>
                            <tr>
                                <td>Faddoul et al.</td>
                                <td>Filter bubbles if user initiates conspiracy</td>
                                <td>2020</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <section class="copyable-section">
                    <h2>The Alternative Influence Network</h2>
                    <p><strong>Rebecca Lewis's</strong> 2018 Data & Society report identified an "Alternative Influence Network" (AIN)&mdash;65 political influencers across 81 YouTube channels promoting reactionary ideologies ranging from libertarianism to white nationalism. <a href="#source-3" class="citation-link" onclick="highlightSource(event, 'source-3')">[3]</a></p>

                    <p>The critical finding: <strong>moderate conservative and libertarian creators frequently host extreme members uncritically</strong>. This collaboration model&mdash;common in YouTube influencer culture&mdash;distributes audiences from mainstream to extreme content through social connections rather than algorithmic recommendations.</p>

                    <p>A large-scale audit study analyzing 330,925 videos across 349 channels found evidence of user migration from <strong>Alt-lite → Intellectual Dark Web → Alt-right</strong>. <a href="#source-7" class="citation-link" onclick="highlightSource(event, 'source-7')">[7]</a> Comment communities increasingly overlapped across right-leaning channels, demonstrating audiences consistently migrating from milder to more extreme content.</p>
                </section>

                <figure class="float-figure copyable-section" id="pathway-chart-wrapper">
                    <div class="chart-wrapper">
                        <div class="chart-header">
                            <div class="chart-title">Radicalization Pathway Analysis</div>
                        </div>
                        <canvas id="pathwayChart" height="220"></canvas>
                    </div>
                    <figcaption>User migration patterns from audit study of 330,925 videos</figcaption>
                </figure>

                <section class="copyable-section">
                    <h2>Off-Platform: The Hidden Driver</h2>
                    <p>Perhaps the most significant recent finding comes from <strong>Northeastern University</strong> research published in 2024. Tracking 1,000+ U.S. residents over six months, researchers found users see <strong>more YouTube videos off-platform than on YouTube itself</strong>. <a href="#source-4" class="citation-link" onclick="highlightSource(event, 'source-4')">[4]</a></p>

                    <p>Key insights from lead researcher <strong>Christo Wilson</strong>:</p>
                    <ul>
                        <li>Subscriptions and external referrals&mdash;not the algorithm&mdash;drive users to extremist content</li>
                        <li>Right-leaning websites embed more problematic YouTube channels than centrist/left sites</li>
                        <li><strong>Off-platform viewing leads to on-platform seeking</strong>: users exposed to extremist videos on sites like Breitbart subsequently search for similar content on YouTube</li>
                        <li>YouTube's recommendation algorithm changes (post-2019) appear effective <em>within</em> the platform</li>
                    </ul>

                    <p>This suggests the locus of radicalization may be the broader information ecosystem rather than YouTube's algorithm specifically. Partisan websites serve as entry points, with YouTube functioning as a video repository.</p>
                </section>

                <section class="copyable-section">
                    <h2>The Caleb Cain Case Study</h2>
                    <p>Journalist <strong>Kevin Roose's</strong> investigation of <strong>Caleb Cain</strong>&mdash;a young man who consumed hundreds of far-right YouTube videos&mdash;became influential in shaping the radicalization narrative. <a href="#source-9" class="citation-link" onclick="highlightSource(event, 'source-9')">[9]</a> Cain's journey from self-help videos to Stefan Molyneux to broader far-right content provided a compelling personal story.</p>

                    <p>However, the case also illustrates important nuances:</p>
                    <ul>
                        <li>Cain watched hundreds of far-right videos but <strong>never adopted the most extreme views</strong> (Holocaust denial, white ethnostate)</li>
                        <li>He later <strong>de-radicalized</strong> by consuming left-wing content&mdash;essentially moving to an opposite "rabbit hole"</li>
                        <li>This demonstrates <strong>user agency and reversibility</strong>&mdash;the pathway isn't deterministic</li>
                    </ul>

                    <p>Critics noted the investigation named specific YouTubers (Rubin, Shapiro) as part of a radicalization pipeline, drawing accusations of "guilt by association" that complicated the public discourse.</p>
                </section>

                <div class="info-box cyan copyable-section" style="clear: both;">
                    <div class="info-title"><i data-lucide="info"></i>Filter Bubble Effect</div>
                    <p><strong>Faddoul et al. (2020)</strong> found a conditional filter bubble: YouTube's recommender promotes conspiracy content <em>only if</em> users begin by watching conspiracy content. <a href="#source-8" class="citation-link" onclick="highlightSource(event, 'source-8')">[8]</a> Users who start with mainstream content are not pushed toward conspiracies. This suggests user choice initiates the pathway.</p>
                </div>

                <section class="copyable-section">
                    <h2>Platform Response and Enforcement</h2>
                    <p>YouTube's transparency reports show aggressive enforcement: in the first half of 2024, the platform removed <strong>16.8 million videos</strong> for guideline violations, including <strong>563,506 specifically for violent extremism</strong>. <a href="#source-6" class="citation-link" onclick="highlightSource(event, 'source-6')">[6]</a></p>

                    <p>After implementing machine learning systems in 2017, <strong>over 50% of violent extremism removals</strong> now occur before videos reach 10 views&mdash;up from 8% previously. YouTube maintains partnerships with 150+ academics and NGOs, including King's College London's International Centre for the Study of Radicalisation.</p>

                    <p>Congressional testimony by YouTube CPO <strong>Neal Mohan</strong> in 2022 outlined the platform's <em>"4 Rs"</em> approach: <strong>Remove</strong> violating content, <strong>Raise</strong> authoritative voices, <strong>Reward</strong> trusted creators, <strong>Reduce</strong> borderline content recommendations. <a href="#source-11" class="citation-link" onclick="highlightSource(event, 'source-11')">[11]</a></p>
                </section>

                <section class="copyable-section">
                    <h2>Methodological Limitations</h2>
                    <p>The lack of consensus across 23+ studies stems from significant methodological variations:</p>
                    <ul>
                        <li><strong>Recommendation audits vs. user tracking:</strong> Studies analyzing recommendations in isolation may not reflect actual viewing patterns</li>
                        <li><strong>Limited external data access:</strong> Only YouTube has complete recommendation logs; researchers use proxies</li>
                        <li><strong>Temporal changes:</strong> YouTube modified its algorithm multiple times (2017, 2019); studies conducted at different periods measure different systems</li>
                        <li><strong>Definition of "radicalization":</strong> Studies vary in whether they measure exposure to extreme content vs. actual belief change</li>
                    </ul>

                    <p>Research on algorithmic debiasing demonstrates that intervention methods can minimize ideological bias in recommendations, but with <strong>differential effectiveness</strong>&mdash;debiasing proves more challenging for right-leaning users. <a href="#source-12" class="citation-link" onclick="highlightSource(event, 'source-12')">[12]</a></p>
                </section>

                <section class="copyable-section">
                    <h2>Synthesis: What the Evidence Shows</h2>
                    <p>Integrating across studies, the evidence suggests:</p>
                    <ul>
                        <li><strong>The algorithm's direct radicalization effect is limited</strong>&mdash;users seek content matching existing beliefs</li>
                        <li><strong>Network effects matter more:</strong> Influencer collaborations and social connections distribute audiences to extreme content</li>
                        <li><strong>Off-platform exposure is critical:</strong> Partisan websites embedding extremist content may drive more radicalization than YouTube's algorithm</li>
                        <li><strong>Platform changes appear effective within YouTube</strong>&mdash;post-2019 recommendation modifications reduced conspiracy promotion</li>
                        <li><strong>Filter bubbles are user-initiated:</strong> The algorithm amplifies existing interests rather than creating new ones</li>
                    </ul>
                </section>

                <div class="info-box green copyable-section">
                    <div class="info-title"><i data-lucide="check-circle"></i>Key Takeaways</div>
                    <p><strong>For researchers:</strong> Methodological standardization is needed; user-tracking studies provide more reliable evidence than recommendation audits.</p>
                    <p><strong>For platforms:</strong> YouTube's internal changes appear effective; the problem may lie in off-platform embedding and cross-platform dynamics.</p>
                    <p><strong>For policymakers:</strong> Focusing solely on recommendation algorithms may miss the larger information ecosystem where radicalization actually occurs.</p>
                </div>

            </article>
        </div>
    </div>

    <div id="footer-placeholder"></div>

    <script>
        window.addEventListener('gv:componentsReady', () => { lucide.createIcons(); });

        function highlightSource(event, id) {
            event.preventDefault();
            const element = document.getElementById(id);
            if (element) {
                element.scrollIntoView({ behavior: 'smooth', block: 'center' });
                element.classList.remove('highlight');
                void element.offsetWidth;
                element.classList.add('highlight');
            }
        }

        function shareToTwitter() {
            const url = encodeURIComponent(window.location.href);
            const text = encodeURIComponent(document.title);
            window.open(`https://twitter.com/intent/tweet?url=${url}&text=${text}`, '_blank', 'width=600,height=400');
        }
        function shareToFacebook() {
            const url = encodeURIComponent(window.location.href);
            window.open(`https://www.facebook.com/sharer/sharer.php?u=${url}`, '_blank', 'width=600,height=400');
        }
        function shareToLinkedIn() {
            const url = encodeURIComponent(window.location.href);
            window.open(`https://www.linkedin.com/sharing/share-offsite/?url=${url}`, '_blank', 'width=600,height=600');
        }
        function copyShareLink() {
            navigator.clipboard.writeText(window.location.href).then(() => {
                const btn = document.getElementById('copyLinkBtn');
                const originalHTML = btn.innerHTML;
                btn.innerHTML = '<i data-lucide="check"></i> Copied!';
                btn.classList.add('copied');
                lucide.createIcons({ nodes: [btn] });
                setTimeout(() => { btn.innerHTML = originalHTML; btn.classList.remove('copied'); lucide.createIcons({ nodes: [btn] }); }, 2000);
            });
        }

        // Studies Chart
        new Chart(document.getElementById('studiesChart').getContext('2d'), {
            type: 'doughnut',
            data: {
                labels: ['Implicate Algorithm (14)', 'Mixed Results (7)', 'No Effect (2)'],
                datasets: [{
                    data: [14, 7, 2],
                    backgroundColor: ['rgba(239, 68, 68, 0.8)', 'rgba(245, 158, 11, 0.8)', 'rgba(16, 185, 129, 0.8)'],
                    borderColor: ['#ef4444', '#f59e0b', '#10b981'],
                    borderWidth: 2
                }]
            },
            options: {
                responsive: true,
                plugins: { legend: { display: true, position: 'bottom', labels: { color: '#94a3b8', padding: 15 } } }
            }
        });

        // Pathway Chart
        new Chart(document.getElementById('pathwayChart').getContext('2d'), {
            type: 'bar',
            data: {
                labels: ['Mainstream Media', 'Alt-lite', 'IDW', 'Alt-right'],
                datasets: [{
                    label: 'User Migration Pattern',
                    data: [100, 72, 48, 23],
                    backgroundColor: ['rgba(16, 185, 129, 0.8)', 'rgba(6, 182, 212, 0.8)', 'rgba(245, 158, 11, 0.8)', 'rgba(239, 68, 68, 0.8)'],
                    borderColor: ['#10b981', '#06b6d4', '#f59e0b', '#ef4444'],
                    borderWidth: 1
                }]
            },
            options: {
                responsive: true,
                plugins: { legend: { display: false } },
                scales: {
                    y: { beginAtZero: true, grid: { color: 'rgba(255,255,255,0.05)' }, ticks: { color: '#64748b', callback: v => v + '%' } },
                    x: { grid: { display: false }, ticks: { color: '#94a3b8' } }
                }
            }
        });
    </script>
</body>
</html>
